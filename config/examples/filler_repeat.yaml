# Example: Filler tokens mode with problem repetition
# Based on Redwood Research blog post:
# https://blog.redwoodresearch.org/p/recent-llms-can-use-filler-tokens
# Repeating the problem statement generally works better and is more reliable
# than filler tokens, especially for relatively weaker models.
model:
  provider: "anthropic"
  name: "claude-3-5-sonnet-20241022"

cipher:
  type: "filler"  # Filler token mode - no CoT, only final answer with filler tokens
  params: {}
  filler:
    type: "repeat"  # Options: "counting", "dots", "lorem", "repeat"
    count: 5  # Number of times to repeat the problem

prompt:
  num_examples: 5

dataset:
  name: "math500"
  num_test_examples: 100
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

experiment:
  output_dir: "results"

